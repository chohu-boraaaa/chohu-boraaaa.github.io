---
layout: post  
title: "[데이터마이닝] 3장. 로지스틱 회귀"
date: 2022-10-29
excerpt: "R 데이터마이닝의 3장 내용을 정리하였습니다."
tag:
- 데이터마이닝
comments: true
--- 

# 3.1 서론

---

- 로지스틱 회귀 : 반응변수가 범주형인 경우(이진형인 경우가 많음) 적용되는 회귀분석모형
- 예측모형 : 새로운 설명변수(예측변수) 값이 주어질 때 반응변수의 각 범주에 속할 확률을 추정
- 분류모형 : 추정확률의 기준치에 따라 분류의 목적으로 사용될 수 있음
- 사후확률 : 모형의 적합을 통해 추정된 확률

# 3.2 로지스틱 회귀

---

- 이진반응변수 Y에 대해, 다중 로지스틱 회귀모형의 일반적 형태
- y가 범주형인데 일반 회귀적합을 하면 (0 or 1 = -무한대 ~ 무한대)의 양변의 구조적 문제가 발생하여 오류 -> log(오즈)로 구조적 문제 해결 가능
- $log(오즈)$

![](https://user-images.githubusercontent.com/77424107/198838476-bf6db2b2-bde4-4ece-aa18-850b2e91052b.png)
![](https://user-images.githubusercontent.com/77424107/198838544-a1885c06-b65f-4acd-99cd-7f536d81085d.png)

## ✅ 장점 : 오즈의 관점에서 해석 가능
- $exp(\beta_1)$의 의미 : 나머지 변수($x_2, ..., x_k$)가 주어질 때 $x_1$이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지를 나타내는 값

## ✅ 다음의 방법으로 재표현 가능

![](https://user-images.githubusercontent.com/77424107/198838897-89e88ed1-7c2d-4a3b-ab9d-9c87795a2f6a.png)

## ✅ 예측 변수가 1개 vs 2개인 경우 로지스틱 회귀 적합한 결과 시각화

![](https://user-images.githubusercontent.com/77424107/198839127-22c15038-9437-48ac-99fb-7a12bd1d9160.png)

### ✔️ 로지스틱 회귀식에 대한 의미 해석

#### 1️⃣ 위 식은 다중 로지스틱 회귀 함수에 해당
- 그래프 형태 : 설명변수가 한 개인 경우엔 $\beta_1$의 부호에 따라 결정
- $\beta_1$>0 : S자 모양
- $\beta_0$<0 : 역S자 모양

#### 2️⃣ 표준 로지스틱 분포의 누적분포함수(cdf)를 $F(x)$라고 할 때 동일한 표현
- 표준로지스틱 분포 = $\alpha=0$, $\beta=1$로 표준화한 로지스틱 분포 (L(0,1))
$$F(x)=\frac{e^x}{1+e^x}$$

![](https://user-images.githubusercontent.com/77424107/198839574-86c94768-0bcd-491f-946d-c644dfe1a717.png)

### ✔️ 프로빗 모형
- 로지스틱 회귀모형과 유사
- $F(.)$ 대신 표준정규분포의 누적함수 $\phi(.)$로 성공의 확률을 모형화한 것
$$\phi^{-1}(\pi(x))=\alpha+\beta_1x_1+...+\beta_kx_k$$

### ✔️ 로지스틱 회귀의 이용
- 분류의 목적으로 사용될 경우엔 $\pi(x)$가 기준값(예. 1/2)보다 크면 Y=1인 집단으로, 작으면 Y=0인 집단으로 분류
- 분류 기준값의 결정은 사전정보 또는 손실함수를 이용하거나 (정분류율, 민감도, 특이도)를 동시에 고려하는 등 다양한 방법이 사용됨

### ✔️ R에서 로지스틱 회귀모형 적합
- glm() 함수

### 예제1) 반응변수의 범주가 2개인 로지스틱 회귀를 적용하기 위해 iris 자료의 일부분만 이용하기로 함. Species가 setosa와 versicolor인 100개 자료만을 이용.
```
> data(iris)
> a <- subset(iris, Species == "setosa" | Species == "versicolor")
> a$Species <- factor(a$Species )
> str(a)
'data.frame':	100 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 2 levels "setosa","versicolor": 1 1 1 1 1 1 1 1 1 1 ... 
```
- 이 결과에서 Species는 Factor형 변수
- setosa : Y=1
- versicolor : Y=2
- 로지스틱 회귀가 적용될 때, 보다 큰 숫자인 versicolor일 오즈를 모형화하므로 해석에 유의할 필요가 있음
```
> b <- glm(Species~Sepal.Length, data=a, family=binomial) # check option family # glm() 함수 이용하여 로지스틱 회귀모형 적합
> summary(b) # summary model

Call:
glm(formula = Species ~ Sepal.Length, family = binomial, data = a)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.05501  -0.47395  -0.02829   0.39788   2.32915  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)   -27.831      5.434  -5.122 3.02e-07 ***
Sepal.Length    5.140      1.007   5.107 3.28e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.629  on 99  degrees of freedom
Residual deviance:  64.211  on 98  degrees of freedom
AIC: 68.211

Number of Fisher Scoring iterations: 6
```
- 회귀계수의 검정에서 p-값이 거의 0이므로 **Sepal.Length**가 매우 유의한 변수이며, Sepal.Length가 한 단위 증가함에 따라 versicolor(Y=2)일 오즈가 exp(5.140), 대략 170배 증가함을 알 수 있다.
- Null deviance : 절편만 포함하는 모형($H_0:\beta=0$하의 모형)의 완전모형으로부터의 이탈도(deviance), 위 결과에서는 p-값=$P(\chi^2(99)>138.629)=0.005$으로 통계적으로 유의하므로 **적합 결여**를 나타냄.
- Residual deviance : 예측변수 Sepal.Length가 추가된 적합모형의 이탈도. Null deviance에 비해 자유도 1 기준에 이탈도 감소가 74.4 정도의 큰 감소를 보이며 p-값=$P(\chi^2(98)>64.211)=0.997$이므로 귀무가설(적합모형)이 기각되지 않고 잘 적합하고 있다고 할 수 있다.
- 완전모형(=포화모형)에 100% 설명력을 가짐 = 통계적으로 무의미함
```
> coef(b) # 회귀계수 베타
 (Intercept) Sepal.Length 
  -27.831451     5.140336 
> exp(coef(b)["Sepal.Length"]) # odds의 증가량 exp(베타)
Sepal.Length 
    170.7732 
> confint(b, parm = 'Sepal.Length') # 회귀계수 베타 신뢰구간
프로파일링이 완료되길 기다리는 중입니다...
   2.5 %   97.5 % 
3.421613 7.415508 
> exp(confint(b, parm= 'Sepal.Length')) # odds의 증가량 exp(베타) 신뢰구간
프로파일링이 완료되길 기다리는 중입니다...
     2.5 %     97.5 % 
  30.61878 1661.55385 
> fitted(b)[c(1:5, 96:100)] # fitted 함수를 통해 적합결과 확인
         1          2          3          4          5         96         97 
0.16579367 0.06637193 0.02479825 0.01498061 0.10623680 0.81282396 0.81282396 
        98         99        100 
0.98268360 0.16579367 0.81282396 
> # predict() 함수 이용하여 새로운 자료에 대한 예측 수행
> # 여기에서는 편의상 모형구축에 사용된 자료 재사용
> predict(b, newdata=a [c(1, 50, 51, 100), ], type="response" )
        1        50        51       100 
0.1657937 0.1062368 0.9997116 0.8128240 
```

#### 💜 cdplot() 함수 : Sepal.Length(연속형)의 변화에 따른 범주형 변수의 조건부 분포를 보여줌. 
```
> cdplot(Species~Sepal.Length, data=a)
```
![](https://user-images.githubusercontent.com/77424107/198864599-2396df51-22bf-4422-83d4-6c0a3235d4f3.png)

- Sepal.Legnth가 커짐에 따라 아래쪽 영역인 versicolor의 확률이 증가
- cdplot() 함수는 로지스틱 회귀의 탐색적 분석에 유용

#### 💜 적합된 로지스틱 회귀모형 그래프
```
> plot(a$Sepal.Length, a$Species, xlab="Sepal.Length")
> x=seq(min(a$Sepal.Length), max(a$Sepal.Length), 0.1)
> lines(x, 1+(1/(1+(1/exp(-27.831+5.140*x)))), type="l", col="red")
```
![](https://user-images.githubusercontent.com/77424107/198864768-13d7c636-910b-4c4c-b138-1454a8c6c69d.png)
- 그래프 그릴 때 $\pi(x)$에 1+ 를 추가해준 이유 : 추정된 확률의 범위를 (0,1)에서 (1,2)로 변경해주기 위함

### 예제 2) 예측변수가 여러 개인 다중 로지스틱 회귀모형 적합 : 분석에 사용되는 자료는 1973~1974도에 생산된 32종류의 자동차에 대해 11개 변수를 측정한 자료
```
> attach(mtcars)
> str(mtcars)
'data.frame':	32 obs. of  11 variables:
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
 $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
 $ disp: num  160 160 108 258 360 ...
 $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
 $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
 $ qsec: num  16.5 17 18.6 19.4 17 ...
 $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
 $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
```
- 이항변수 vs를 반응변수로, mpg와 am을 예측변수로 하는 로지스틱 회귀모형 적합
```
> # family=binomial는 family=binomial(logit)과 동일
> glm.vs <- glm(vs~mpg+am, data=mtcars, family=binomial) 
> summary(glm.vs)

Call:
glm(formula = vs ~ mpg + am, family = binomial, data = mtcars)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.05888  -0.44544  -0.08765   0.33335   1.68405  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -12.7051     4.6252  -2.747  0.00602 **
mpg           0.6809     0.2524   2.698  0.00697 **
am           -3.0073     1.5995  -1.880  0.06009 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 20.646  on 29  degrees of freedom
AIC: 26.646

Number of Fisher Scoring iterations: 6
```

#### 💜 추정된 회귀계수 $\hat{\beta_1}$에 대한 해석
- 다른 모든 변수들이 주어질 때, mpg 값이 한 단위 증가함에 따라 vs가 1일 오즈가 exp(0.6809) = 1.98배(98%) 증가
- 다른 모든 변수들이 주어질 때, am 값이 한 단위 증가함에 따라 vs가 1일 오즈가 exp(-3.0073) = 0.05배 증가 = 변속기가 수동인 경우 자동에 비해 vs=1의 오즈가 95% 감소

#### 💜 예측변수가 여러 개인 모형 적합시 변수선택법 적용
- direction= 옵션 : 'both'(stepwise), 'backward'(디폴트, 후진선택법), 'forward'(전진선택법)
- 전진선택법 : 절편항만 포함하는 가장 작은 모형에서 반응변수에 가장 큰 영향을 주는 설명변수를 차례로 모형에 포함시켜나가되 더 이상 의미 있는 변수가 없을 때 중단하는 방법
- 후진제거법 : 모든 설명변수를 포함하는 모형에서 기여도가 낮은 변수를 차례로 제거해 나가되 더 이상 제거할 변수가 없으면 중단하는 기법
- 단계별 선택법 : 전진선택법에서 한 번 선택된 변수는 다음 단계에서 제거될 기회를 갖지 못하고, 후진제거법에서 한 번 제거된 변수는 다음 단계에서 선택될 기회를 갖지 못한다. 이런 단점을 보완하고자 먼저 선택된 변수도 다음 단계에서 제거될 수 있도록 변수선택의 단계마다 체크하는 방법.
```
> step.vs <- step(glm.vs, direction="backward")
Start:  AIC=26.65
vs ~ mpg + am

       Df Deviance    AIC
<none>      20.646 26.646
- am    1   25.533 29.533
- mpg   1   42.953 46.953
> summary(step.vs) # summary(glm.vs)와 결과 동일

Call:
glm(formula = vs ~ mpg + am, family = binomial, data = mtcars)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.05888  -0.44544  -0.08765   0.33335   1.68405  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -12.7051     4.6252  -2.747  0.00602 **
mpg           0.6809     0.2524   2.698  0.00697 **
am           -3.0073     1.5995  -1.880  0.06009 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 20.646  on 29  degrees of freedom
AIC: 26.646

Number of Fisher Scoring iterations: 6

```

#### 💜 glm() 함수의 유용한 수행 결과들 예시
```
> ls(glm.vs)
 [1] "aic"               "boundary"          "call"             
 [4] "coefficients"      "contrasts"         "control"          
 [7] "converged"         "data"              "deviance"         
[10] "df.null"           "df.residual"       "effects"          
[13] "family"            "fitted.values"     "formula"          
[16] "iter"              "linear.predictors" "method"           
[19] "model"             "null.deviance"     "offset"           
[22] "prior.weights"     "qr"                "R"                
[25] "rank"              "residuals"         "terms"            
[28] "weights"           "xlevels"           "y"                
> str(glm.vs)
List of 30
 $ coefficients     : Named num [1:3] -12.705 0.681 -3.007
  ..- attr(*, "names")= chr [1:3] "(Intercept)" "mpg" "am"
 $ residuals        : Named num [1:32] -1.24 -1.24 2.21 1.15 -2.03 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ fitted.values    : Named num [1:32] 0.196 0.196 0.453 0.866 0.507 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ effects          : Named num [1:32] 0.726 -1.939 1.88 0.932 -0.658 ...
  ..- attr(*, "names")= chr [1:32] "(Intercept)" "mpg" "am" "" ...
 $ R                : num [1:3, 1:3] -1.8 0 0 -35.07 -5.29 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "(Intercept)" "mpg" "am"
  .. ..$ : chr [1:3] "(Intercept)" "mpg" "am"
 $ rank             : int 3
 $ qr               :List of 5
  ..$ qr   : num [1:32, 1:3] -1.804 0.22 0.276 0.189 0.277 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
  .. .. ..$ : chr [1:3] "(Intercept)" "mpg" "am"
  ..$ rank : int 3
  ..$ qraux: num [1:3] 1.22 1.1 1.12
  ..$ pivot: int [1:3] 1 2 3
  ..$ tol  : num 1e-11
  ..- attr(*, "class")= chr "qr"
 $ family           :List of 12
  ..$ family    : chr "binomial"
  ..$ link      : chr "logit"
  ..$ linkfun   :function (mu)  
  ..$ linkinv   :function (eta)  
  ..$ variance  :function (mu)  
  ..$ dev.resids:function (y, mu, wt)  
  ..$ aic       :function (y, n, mu, wt, dev)  
  ..$ mu.eta    :function (eta)  
  ..$ initialize: language {     if (NCOL(y) == 1) { ...
  ..$ validmu   :function (mu)  
  ..$ valideta  :function (eta)  
  ..$ simulate  :function (object, nsim)  
  ..- attr(*, "class")= chr "family"
 $ linear.predictors: Named num [1:32] -1.4131 -1.4131 -0.1874 1.8666 0.0281 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ deviance         : num 20.6
 $ aic              : num 26.6
 $ null.deviance    : num 43.9
 $ iter             : int 6
 $ weights          : Named num [1:32] 0.157 0.157 0.248 0.116 0.25 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ prior.weights    : Named num [1:32] 1 1 1 1 1 1 1 1 1 1 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ df.residual      : int 29
 $ df.null          : int 31
 $ y                : Named num [1:32] 0 0 1 1 0 1 0 1 1 1 ...
  ..- attr(*, "names")= chr [1:32] "Mazda RX4" "Mazda RX4 Wag" "Datsun 710" "Hornet 4 Drive" ...
 $ converged        : logi TRUE
 $ boundary         : logi FALSE
 $ model            :'data.frame':	32 obs. of  3 variables:
  ..$ vs : num [1:32] 0 0 1 1 0 1 0 1 1 1 ...
  ..$ mpg: num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
  ..$ am : num [1:32] 1 1 1 0 0 0 0 0 0 0 ...
  ..- attr(*, "terms")=Classes 'terms', 'formula'  language vs ~ mpg + am
  .. .. ..- attr(*, "variables")= language list(vs, mpg, am)
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "vs" "mpg" "am"
  .. .. .. .. ..$ : chr [1:2] "mpg" "am"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "mpg" "am"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
  .. .. ..- attr(*, "predvars")= language list(vs, mpg, am)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "numeric"
  .. .. .. ..- attr(*, "names")= chr [1:3] "vs" "mpg" "am"
 $ call             : language glm(formula = vs ~ mpg + am, family = binomial, data = mtcars)
 $ formula          :Class 'formula'  language vs ~ mpg + am
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
 $ terms            :Classes 'terms', 'formula'  language vs ~ mpg + am
  .. ..- attr(*, "variables")= language list(vs, mpg, am)
  .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ : chr [1:3] "vs" "mpg" "am"
  .. .. .. ..$ : chr [1:2] "mpg" "am"
  .. ..- attr(*, "term.labels")= chr [1:2] "mpg" "am"
  .. ..- attr(*, "order")= int [1:2] 1 1
  .. ..- attr(*, "intercept")= int 1
  .. ..- attr(*, "response")= int 1
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
  .. ..- attr(*, "predvars")= language list(vs, mpg, am)
  .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "numeric"
  .. .. ..- attr(*, "names")= chr [1:3] "vs" "mpg" "am"
 $ data             :'data.frame':	32 obs. of  11 variables:
  ..$ mpg : num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
  ..$ cyl : num [1:32] 6 6 4 6 8 6 8 4 4 6 ...
  ..$ disp: num [1:32] 160 160 108 258 360 ...
  ..$ hp  : num [1:32] 110 110 93 110 175 105 245 62 95 123 ...
  ..$ drat: num [1:32] 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
  ..$ wt  : num [1:32] 2.62 2.88 2.32 3.21 3.44 ...
  ..$ qsec: num [1:32] 16.5 17 18.6 19.4 17 ...
  ..$ vs  : num [1:32] 0 0 1 1 0 1 0 1 1 1 ...
  ..$ am  : num [1:32] 1 1 1 0 0 0 0 0 0 0 ...
  ..$ gear: num [1:32] 4 4 4 3 3 3 3 4 4 4 ...
  ..$ carb: num [1:32] 4 4 1 1 2 1 4 2 2 4 ...
 $ offset           : NULL
 $ control          :List of 3
  ..$ epsilon: num 1e-08
  ..$ maxit  : num 25
  ..$ trace  : logi FALSE
 $ method           : chr "glm.fit"
 $ contrasts        : NULL
 $ xlevels          : Named list()
 - attr(*, "class")= chr [1:2] "glm" "lm"
```

#### 💜 anova() 함수 : 모형의 적합(변수가 추가되는) 단계별로 이탈도의 감소량과 유의성검증 결과를 제시
```
> anova(glm.vs, test="Chisq")
Analysis of Deviance Table

Model: binomial, link: logit

Response: vs

Terms added sequentially (first to last)


     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                    31     43.860              
mpg   1   18.327        30     25.533 1.861e-05 ***
am    1    4.887        29     20.646   0.02706 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
- 절편항만을 포함하는 영모형에서 mpg, am 변수가 차례로 추가됨에 따라 발생하는 이탈도 감소량 제시
- p-값은 각각 $P(\chi^2(1)>18.327)$과 $P(\chi^2(1)>4.887)$을 계산한 값
- 두 변수가 차례로 추가되면서 생겨나는 이탈도의 감소량이 모두 통계적으로 유의함
```
> 1-pchisq(18.327, 1) # p-value
[1] 1.860515e-05
> 1-pchisq(4.887, 1) # p-value
[1] 0.02705967
```

# 참고 1) 로지스틱 회귀모형 = 로짓 모형
- 일반화선형모형의 특별한 경우
- 반응변수의 범주가 3개 이상인 경우에는 범주의 유형(명목형 또는 순서형)에 따라 다양한 다범주 로짓모형 적합 가능

# 참고 2) 일반화선형모형(GLM)에서의 이탈도(deviance)
- Null Deviance : 절편모형의 완전모형으로부터의 이탈도
$$Null Deviance = 2[LL(포화모형)-LL(영모형)], df=n-1$$
- Residual Deviance : 제안모형의 완전모형으로부터의 이탈도
$$Residual Deviance = 2[LL(포화모형)-LL(제안모형)], df=n-(p+1)$$
- LL = 로그가능도
- 이탈도 값이 작을수록 해당모형이 자료를 잘 적합함을 의미
- 검정: 두 종류의 Deviance가 해당모형이 참일 때, 근사적으로 카이제곱분포를 따른다는 사실에 기초하고 **자유도는 (포화모형 모수의 수 - 해당 모형 모수의 수)**

## ✅ 영모형과 제안모형 간 비교(검정)
- (Null Deviance - Proposed Deviance)가 근사적으로 자유도가 (n-(p+1))-(n-1)=p인 카이제곱분포를 따른다는 사실에 기초

## 1️⃣ 포화모형(Saturated Model)
- 추정해야 할 모수의 수가 데이터 수와 동일한 모형
- 완전 모형(Full Model)

## 2️⃣ 영모형(Null Model)
- 절편항만 가지는 모형
- 추정할 모수가 1개

## 3️⃣ 제안모형(Purposed Model)
- (p개의 모수+절편항)을 포함하는 모형
- 추정할 모수가 p+1개 

# 참고 3) 로짓 변환
![](https://bokyeong-kim.github.io/assets/img/logit(3).png)