---
layout: post  
title: "[데이터마이닝] 5장. 단순 베이즈 분류"
date: 2022-10-30
excerpt: "R 데이터마이닝의 5장 내용을 정리하였습니다."
tag:
- 데이터마이닝
comments: true
--- 

# 5.1 서론

---

- 단순 베이즈 분류(naive Bayes classification) 모형은 베이즈 정리에 기반한 방법
- 사후확률(일종의 조건부 결합확률) 계산시 조건부 독립을 가정하여 계산을 단순화한 방법
- 사후확률이 큰 집단으로 새로운 데이터 분류
- 조건부 독립의 가정이 비현실적 측면이 있지만 계산이 간편하여 널리 이용됨
- 문서분류(spam or legitimate, sports or politics 등), 의료진단 등에 많이 사용
- 적절한 전처리 과정을 거친 단순 베이즈 분류는 서포트벡터머신을 포함한 보다 발전된 기법과도 경쟁

# 5.2 단순 베이즈 분류

---

- 연속형 또는 이산형에 관계없이 임의 크기의 예측변수 다룰 수 있음
- 데이터가 다음과 같이 주어질 때
$$x=(x_1, x_2, ..., x_d)$$
- 이 데이터가 C_j 집단으로부터 나왔을 사후확률은 베이즈 정리로부터 다음과 같다.
![](https://user-images.githubusercontent.com/77424107/198889294-bd0c1859-7534-4ae3-9196-868e725e5fa5.png)
- 위의 사후확률의 계산을 좀 더 편하게 할 수 있도록 예측변수들 간 독립을 가정
![](https://user-images.githubusercontent.com/77424107/198889378-6cff5398-3d6e-4d34-9ecc-c147b16ed8a9.png)
- 위의 식을 이용하여 **사후확률의 분자를 계산하고, 그 결과를 이용하여 분류를 수행**

## 사례 1) 문서분류와 관련된 예제
![](https://user-images.githubusercontent.com/77424107/198890376-1d521669-6fe8-4c91-ad68-6070035d9acf.png)

- 따라서, 입력문서는 사후확률이 더 큰 action으로 분류됨

### ❗주의 : 낮은-빈도 문제(low-frequency problem)
- 위의 예에서 comdey에서 furious 단어 빈도가 0이므로, furious 단어를 포함하는 새로운 자료는 항상 사후확률이 0이 되어 버림.
- 이 문제를 해결하기 위해 **모든 속성값-군집 조합에 대한 빈도에 작은 수를 더해주어 계산 수행**

## 사례 2) 여러 개 연속형 예측변수를 가지는 경우
```
> (height1 <- dnorm(6, mean=5.855, sd=sqrt(0.035033)))
[1] 1.578886
> weight1 <- dnorm(130, mean=176.26, sd=sqrt(122.92))
> feet1 <- dnorm(8, mean=11.25, sd=sqrt(0.91667))
> m.x <- 0.5*height1*weight1*feet1  # p(남성|x)
> m.x
[1] 6.175299e-09
> height2 <- dnorm(6, mean=5.4175, sd=sqrt(0.097225))
> weight2 <- dnorm(130, mean=132.5, sd=sqrt(558.33))
> feet2 <- dnorm(8, mean=7.5, sd=sqrt(1.6667))
> f.x <- 0.5 * height2 * weight2 * feet2 # p(여성|x)
> f.x
[1] 0.0005377879
```
![](https://user-images.githubusercontent.com/77424107/198890729-4d794f8d-8214-4ebd-98d8-f24c36eb8a26.png)

## 예제 1) 패키지 {e1017} 이용 : 단순 베이즈 분석 - iris 자료
```
> data(iris)
> head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
```
### 💜 R패키지 {e1071}의 naiveBayes() 함수를 이용하여 단순 베이즈 분류 수행
```
> library(e1071)
> m <- naiveBayes(Species ~ ., data = iris)
> m

Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Conditional probabilities:
            Sepal.Length
Y             [,1]      [,2]
  setosa     5.006 0.3524897 # 평균, 표준편차
  versicolor 5.936 0.5161711
  virginica  6.588 0.6358796

            Sepal.Width
Y             [,1]      [,2]
  setosa     3.428 0.3790644
  versicolor 2.770 0.3137983
  virginica  2.974 0.3224966

            Petal.Length
Y             [,1]      [,2]
  setosa     1.462 0.1736640
  versicolor 4.260 0.4699110
  virginica  5.552 0.5518947

            Petal.Width
Y             [,1]      [,2]
  setosa     0.246 0.1053856
  versicolor 1.326 0.1977527
  virginica  2.026 0.2746501
```
### 💜 predict() 함수 이용하여 예측 실시 및 결과를 정오분류표로 나타내기
```
> table(predict(m, iris), iris[,5])
            
             setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         47         3
  virginica       0          3        47
```

## 예제 2) 패키지 {klaR}을 이용하여 단순베이즈 분류 수행 - spam 메일 분류 예제
```
> data(spam)
> str(spam)
'data.frame':	4601 obs. of  58 variables:
 $ A.1 : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
 $ A.2 : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...
 $ A.3 : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
 $ A.4 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.5 : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
 $ A.6 : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...
 $ A.7 : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...
 $ A.8 : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...
 $ A.9 : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...
 $ A.10: num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...
 $ A.11: num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...
 $ A.12: num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...
 $ A.13: num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...
 $ A.14: num  0 0.21 0 0 0 0 0 0 0 0 ...
 $ A.15: num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...
 $ A.16: num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...
 $ A.17: num  0 0.07 0.06 0 0 0 0 0 0 0 ...
 $ A.18: num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...
 $ A.19: num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...
 $ A.20: num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...
 $ A.21: num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...
 $ A.22: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.23: num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...
 $ A.24: num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...
 $ A.25: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.26: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.27: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.28: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.29: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.30: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.31: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.32: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.33: num  0 0 0 0 0 0 0 0 0.15 0 ...
 $ A.34: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.35: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.36: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.37: num  0 0.07 0 0 0 0 0 0 0 0 ...
 $ A.38: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.39: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.40: num  0 0 0.06 0 0 0 0 0 0 0 ...
 $ A.41: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.42: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.43: num  0 0 0.12 0 0 0 0 0 0.3 0 ...
 $ A.44: num  0 0 0 0 0 0 0 0 0 0.06 ...
 $ A.45: num  0 0 0.06 0 0 0 0 0 0 0 ...
 $ A.46: num  0 0 0.06 0 0 0 0 0 0 0 ...
 $ A.47: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.48: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.49: num  0 0 0.01 0 0 0 0 0 0 0.04 ...
 $ A.50: num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...
 $ A.51: num  0 0 0 0 0 0 0 0 0 0 ...
 $ A.52: num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
 $ A.53: num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
 $ A.54: num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...
 $ A.55: num  3.76 5.11 9.82 3.54 3.54 ...
 $ A.56: int  61 101 485 40 40 15 4 11 445 43 ...
 $ A.57: int  278 1028 2259 191 191 54 112 49 1257 749 ...
 $ spam: Factor w/ 2 levels "email","spam": 2 2 2 2 2 2 2 2 2 2 ... # 스팸메일 여부
> library(klaR)
```

### 💜 전체 자료의 2/3을 훈련용 자료로 하여 NaiveBayes() 함수로 단순 베이즈 분류
```
> train.ind <- sample(1:nrow(spam), ceiling(nrow(spam)*2/3),
+                     replace=FALSE)
> nb.res <- NaiveBayes(spam ~ ., data=spam[train.ind,])
> # 결과
> opar <- par(mfrow=c(2,4))
> plot(nb.res)
```
![](https://user-images.githubusercontent.com/77424107/198974731-fc22e543-2716-42e3-aa1e-9d81ab2cee13.png)
- 위의 결과(그림)는 57개의 예측변수별 분포를 문서의 종류별(spam, non-spam)로 그린 것
- 새로운 자료가 주어질 때, 사후확률은 사전확률과 위 확률들의 곱을 통해 구할 수 있음

### 💜 분석에 제외된 검증용 자료를 이용하여 모형의 정확도 파악
```
> par(opar)
> nb.pred <- predict(nb.res, spam[-train.ind,])
There were 50 or more warnings (use warnings() to see the first 50)
> confusion.mat <- table(nb.pred$class, spam[-train.ind,"spam"])
> confusion.mat
       
        email spam # 열 방향 : 실제
  email   522   34 # 행 방향 : 예측
  spam    420  557
> sum(diag(confusion.mat))/sum(confusion.mat)
[1] 0.7038487
```
- 위의 결과로부터 정분류율은 70.3%

## ✅ 단순 베이즈 분류에서의 결측값 처리
- 훈련단계 : 속성값-군집 조합에 대한 빈도 계산 시 결측값을 포함하는 케이스가 제외됨
- 분류단계 : 결측인 속성이 계산과정에서 생략
- 단순 베이즈 분류에서는 결측값에 대한 처리가 매우 유연함
- 모형구축에서는 결측값을 포함하는 케이스를 제외하며, 분류과정에서는 결측 속성에 대한 확률만 계산에서 제외되므로 수행과정에 문제 없음

## 예제 3) 결측값을 포함하는 자료에 대한 단순 베이즈 분류 : 미국 하원의원 주요법안 찬반여부 조사한 자료
```
> library (e1071)
> data (HouseVotes84, package="mlbench")
> head(HouseVotes84)
       Class   V1 V2 V3   V4   V5 V6 V7 V8 V9 V10  V11  V12 V13 V14 V15  V16
1 republican    n  y  n    y    y  y  n  n  n   y <NA>    y   y   y   n    y
2 republican    n  y  n    y    y  y  n  n  n   n    n    y   y   y   n <NA>
3   democrat <NA>  y  y <NA>    y  y  n  n  n   n    y    n   y   y   n    n
4   democrat    n  y  y    n <NA>  y  n  n  n   n    y    n   y   n   n    y
5   democrat    y  y  y    n    y  y  n  n  n   n    y <NA>   y   y   y    y
6   democrat    n  y  y    n    y  y  n  n  n   n    n    n   y   y   y    y
> summary(HouseVotes84)
        Class        V1         V2         V3         V4         V5     
 democrat  :267   n   :236   n   :192   n   :171   n   :247   n   :208  
 republican:168   y   :187   y   :195   y   :253   y   :177   y   :212  
                  NA's: 12   NA's: 48   NA's: 11   NA's: 11   NA's: 15  
    V6         V7         V8         V9        V10        V11        V12     
 n   :152   n   :182   n   :178   n   :206   n   :212   n   :264   n   :233  
 y   :272   y   :239   y   :242   y   :207   y   :216   y   :150   y   :171  
 NA's: 11   NA's: 14   NA's: 15   NA's: 22   NA's:  7   NA's: 21   NA's: 31  
   V13        V14        V15        V16     
 n   :201   n   :170   n   :233   n   : 62  
 y   :209   y   :248   y   :174   y   :269  
 NA's: 25   NA's: 17   NA's: 28   NA's:104  
> 
> model <- naiveBayes(Class ~ ., data = HouseVotes84)
> pred <- predict(model, HouseVotes84[,-1])
> tab <- table(pred, HouseVotes84$Class)
> tab # confusion table
            
pred         democrat republican
  democrat        238         13
  republican       29        155
> table(HouseVotes84$Class) # prior

  democrat republican 
       267        168 
> sum(tab[row(tab)==col(tab)])/sum(tab) # acc
[1] 0.9034483
```