---
layout: post  
title: "[데이터마이닝] 4장. 의사결정나무"
date: 2022-10-30
excerpt: "R 데이터마이닝의 4장 내용을 정리하였습니다."
tag:
- 데이터마이닝
comments: true
--- 

# 4.1 서론(의사결정나무 = 나무모형)

---

- 의사결정규칙을 나무 구조로 나타내어 전체 자료를 몇 개의 소집단으로 분류하거나 예측을 수행하는 방법
- 상위 노드로부터 하위노드로 트리구조를 형성하는 매 단계마다 분류변수와 분류 기준값의 선택이 중요
- 상위노드에서의 **분류변수, 분류 기준값**은 이 기준에 의해 분기되는 하위노드에서 **노드(집단) 내에서는 동질성이**, **노드(집단) 간에는 이질성이** 커지도록 선택
- 나무모형 크기는 과대적합 혹은 과소적합이 되지 않도록 합리적 기준에 의해 적당히 조절되어야 함
- 예. 고객 타겟팅, 고객들의 신용점수화, 캠페인 반응분석, 고객행동예측, 고객 세분화 등

# 4.2 의사결정나무

---

- 뿌리노드 : 맨 위, 분류(or 예측) 대상이 되는 모든 자료집단을 포함
- 부모노드(마디) : 상위 노드가 하위노드로 분기될 때의 상위 노드
- 자식노드(마디) : 위에서의 하위노드
- 최종노드(terminal node) : 더 이상 분기되지 않는 노드
- 가지분할(split) : 나무의 가지를 생성하는 과정
- 가지치기(pruning) : 생성된 가지를 잘라내어 모형을 단순화 하는 과정(**과적합 방지**). 분류된 관측값의 비율 또는 MSE(Mean Squared Error) 등을 고려하여 수준의 가지치기 규칙 제공해야 함.
- 정지규칙(stopping rule) : 더 이상 분리가 일어나지 않고 현재의 마디가 끝마디가 되도록 하는 여러 규칙. 최대 나무의 깊이, 자식마디의 최소 관측값 수, 카이제곱통계량, 지니 지수, 엔트로피 지수 등이 있음

## ✅ 의사결정나무 종류
- 목표변수가 이산형인 경우 : 분류나무
- 목표변수가 연속형인 경우 : 회귀나무

### ✔️ 분류나무
- 분류(기준) 변수와 분류 기준값 선택 방법 : 카이제곱 통계량의 p-값, 지니 지수, 엔트로피 지수 등
- 분할이 일어날 때 카이제곱통계량 **p-값은 그 값이 작을수록 자식 노드 간 이질성이 큼**
- 자식노드에서 지니 지수나 엔트로피 지수는 **값이 클수록 자식노드 내의 이질성이 큼 = 불확실성이 큼**, 따라서 이 값들이 **작아지는 방향으로 가지분할 수행**
- **낮은 순수도 = 높은 이질성**

### ⭐ 불확실성 측도(uncertainty measure)

- 식들에서 c는 목표변수의 범주의 수이며, 각 지수의 범위는 c=2인 경우에 해당

### 1️⃣ 지니지수
![](https://user-images.githubusercontent.com/77424107/198869506-4af1522d-d537-4efa-b415-e8e82291bbc6.png)

#### 💜 예시
![](https://user-images.githubusercontent.com/77424107/198869569-3a090cf5-3653-4cea-ba9c-48ed666532ed.png)
 
### 2️⃣ 엔트로피 지수
![](https://user-images.githubusercontent.com/77424107/198869644-0aef7c71-da89-444d-9335-27ecb7ecab7f.png)

### 3️⃣ 정보이득(IA)
$$IA(S,A) = I(S) - I(A)$$
- 상위노드의 엔트로피에서 하위노드의 엔트로피를 뺀 값

## cf) 지니지수와 카이제곱통계량 계산 예시
![](https://user-images.githubusercontent.com/77424107/198883042-c16a71c8-ec4e-433b-8dbe-63adfba0d96d.png)

### ✔️ 회귀나무
- 분류변수와 분류 기준값의 선택방법으로 **F-통계량의 F-값, 분산의 감소량** 등이 사용

#### ⭐ F-통계량
- 일원배치법에서의 검정통계량
- 값이 클수록 오차의 변동에 비해 처리의 변동이 크다는 것 의미 = 자식노드(처리들) 간이 이질적임을 의미
- 이 값이 커지는(p-값은 작아지는) 방향으로 가지분할을 수행

#### ⭐ 분산의 감소량
- 값이 최대화되는 방향으로 가지분할 수행

## ✅ 의사결정나무 분석 과정

### 단계 1. 목표변수와 관계 있는 설명변수들 선택
### 단계 2. 분석목적과 자료의 구조에 따라 적절한 분리기준과 정지규칙을 정하여 의사결정나무 생성
### 단계 3. 부적절한 나뭇가지는 제거(가지치기)
### 단계 4. 이익, 위험, 비용 등을 고려하여 **모형평가** (교차타당성 방법도 사용됨)
### 단계 5. 분류 및 예측 수행

## ✅ 의사결정나무분석 위한 알고리즘과 분류 기준변수의 선택법
![](https://user-images.githubusercontent.com/77424107/198883396-4b5efdde-2df9-4cea-86e1-ea0e15c395dc.png)

### 예제 1) R 패키지 {rpart}의 rpart() 함수를 이용하여 의사결정나무 분석을 수행해보기 (iris 자료 이용)
- rpart(recursive patitioning and regression tree)
```
> library(rpart)
> c <- rpart(Species ~., data=iris) # rpart{rpart}
> c
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

# 100개 중에 setosa가 아닌 것 100개 (각 종별 확률) -> 이런식으로 해결하면 됨
1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
> ls(c)
 [1] "call"                "control"             "cptable"            
 [4] "frame"               "functions"           "method"             
 [7] "numresp"             "ordered"             "parms"              
[10] "splits"              "terms"               "variable.importance"
[13] "where"               "y" 
> plot(c, compress=T, margin=0.3) # 적합된 나무모형 시각화
> text(c, cex=1)
```
![](https://user-images.githubusercontent.com/77424107/198884111-535b604a-07a6-415a-87cb-a88be41b6ac8.png)

#### predict 함수를 이용하여 새로운 자료에 대한 예측 수행. (편의상 모형구축에 사용된 자료를 재대입한 결과 제시)
```
> head(predict(c, newdata=iris, type="class"))
     1      2      3      4      5      6 
setosa setosa setosa setosa setosa setosa 
Levels: setosa versicolor virginica
> tail(predict(c, newdata=iris, type="class"))
      145       146       147       148       149       150 
virginica virginica virginica virginica virginica virginica 
Levels: setosa versicolor virginica
```

#### R 패키지 {rpart.plot} 이용하여 적합된 의사결정나무모형 시각화
```
library(rpart.plot)
prp(c, type=4, extra =2) # prp{rpart.plot}
```
![image](https://user-images.githubusercontent.com/77424107/198884522-d4f8f2ab-2294-4595-a97b-d27a64cef20a.png)
- 해석 : 두 조건(Petal.Length>=2.4와 Petal.Width<1.8m)을 만족하는 노드에서 49/54는 이 노드에 속하는 개체가 54개이며 이 가운데 versicolor이 49임을 나타냄

#### ls() 함수
- $cptable : 트리 크기에 따른 비용-복잡도 모수를 제공하며, 교차타당성오차를 함께 제공.
- prune() 또는 rpart.control() : 가지치기와 트리의 최대 크기를 조절하기 위한 옵션

##### 교차타당성오차를 최소로 하는 트리를 형성하는 과정
- cp : 복잡성
- nsplit : 가지의 분기 수
- rel error : 오류율
- xerror : 교차검증오류
- xstd : 교차 검증 오류의 표준오차
```
> c$cptable # cost-complex parameter and cv eroro
    CP nsplit rel error xerror       xstd
1 0.50      0      1.00   1.14 0.05230679
2 0.44      1      0.50   0.68 0.06096994
3 0.01      2      0.06   0.09 0.02908608
> opt <- which.min(c$cptable[,"xerror"]) # xerror(교차타당성오차)을 최소로 함
> cp <- c$cptable[opt, "CP"]# choose CP(=0.01) with min xerror
> prune.c <- prune(c, cp = cp) # prune with CP (with min xerror)
> plot(prune.c)
> text(prune.c, use.n=T)
```
- 위의 rpart()를 수행한 결과와 동일한 결과

#### {rpart}의 plotcp() 함수 이용 : cp 값을 그림으로 나타낼 수 있음
```
> plotcp(c)
```
![image](https://user-images.githubusercontent.com/77424107/198885469-2cf7c5f1-afce-4439-aa22-d375a7b6cbc0.png)
- 선 하단에서 선에 가까운 포인트의 y축(약 0.01)을 선택. 선 하단의 가장 왼쪽에 위치한 포인트의 cp를 선택하는 것이 바람직.

### 예제 2) **{party}의 ctree()함수 이용하여 적합 : 분석에 사용된 자료는 146명의 전립선 암 환자의 자료(stagec)에서 7개의 예측변수를 이용하여 범주형 반응변수(ploidy)를 예측(또는 분류)
```
> library(party)
> data(stagec)
> str(stagec)
'data.frame':	146 obs. of  8 variables:
 $ pgtime : num  6.1 9.4 5.2 3.2 1.9 4.8 5.8 7.3 3.7 15.9 ...
 $ pgstat : int  0 0 1 1 1 0 0 0 1 0 ...
 $ age    : int  64 62 59 62 64 69 75 71 73 64 ...
 $ eet    : int  2 1 2 2 2 1 2 2 2 2 ...
 $ g2     : num  10.26 NA 9.99 3.57 22.56 ...
 $ grade  : int  2 3 3 2 4 3 2 3 3 3 ...
 $ gleason: int  4 8 7 4 8 7 NA 7 6 7 ...
 $ ploidy : Factor w/ 3 levels "diploid","tetraploid",..: 1 3 1 1 2 1 2 3 1 2 ...
```
#### 💜 결측값 제거
```
> # subsitution missing value
> stagec1<- subset(stagec, !is.na(g2))
> stagec2<- subset(stagec1, !is.na(gleason))
> stagec3<- subset(stagec2, !is.na(eet))
> str(stagec3)
'data.frame':	134 obs. of  8 variables:
 $ pgtime : num  6.1 5.2 3.2 1.9 4.8 3.7 15.9 6.3 2.9 1.5 ...
 $ pgstat : int  0 1 1 1 0 1 0 0 1 1 ...
 $ age    : int  64 59 62 64 69 73 64 65 58 70 ...
 $ eet    : int  2 2 2 2 1 2 2 2 2 2 ...
 $ g2     : num  10.26 9.99 3.57 22.56 6.14 ...
 $ grade  : int  2 3 2 4 3 3 3 3 4 3 ...
 $ gleason: int  4 7 4 8 7 6 7 7 8 8 ...
 $ ploidy : Factor w/ 3 levels "diploid","tetraploid",..: 1 1 1 2 1 1 2 2 2 1 ...
```
- 결측값이 제거된 134개의 자료를 이용하여 모형 적합

#### 💜 모형구축을 위한 훈련용 자료(train data)와 모형의 성능을 검증하기 위한 검증룡 자료(test data)를 70%, 30%로 구성
- 모형 만들기 전에 stagec 자료를 복원추출방법을 이용하여 두 개의 부분집합 training(70%)와 test(30%)로 만들고 결과의 재현성을 위해 random seed를 고정
```
> # divid trining:test=7:3
> set.seed(1234)
> ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
> ind
  [1] 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 2 1
 [38] 1 2 2 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 2 1 2 2 1 1 1 1 2 1 1 1 1 1 2 1 2
 [75] 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2
[112] 1 2 1 1 2 2 1 1 2 2 2 2 2 1 1 1 1 1 1 2 1 1 1
> trainData <- stagec3[ind==1, ] # n=102개
> testData <- stagec3[ind==2, ] # n=32개
```

#### 💜 훈련용 자료에 대해 ctree() 적용
```
> tree <- ctree(ploidy ~ ., data=trainData)
> tree

	 Conditional inference tree with 3 terminal nodes

Response:  ploidy 
Inputs:  pgtime, pgstat, age, eet, g2, grade, gleason 
Number of observations:  102 

1) g2 <= 13.01; criterion = 1, statistic = 49.684
  2) pgstat <= 0; criterion = 0.958, statistic = 7.5
    3)*  weights = 36 
  2) pgstat > 0
    4)*  weights = 15 
1) g2 > 13.01
  5)*  weights = 51 
> plot(tree)
```
![image](https://user-images.githubusercontent.com/77424107/198887694-e4393885-bcdb-4b3d-877c-d133c7b58892.png)
- 최종노드의 막대그래프는 반응변수의 각 범주별 비율을 나타냄

#### 💜 predict() 함수를 통해 검증용 자료에 대해 적합모형 적용
```
> testPred = predict(tree, newdata=testData)
> table(testPred, testData$ploidy) # confusion table
            
testPred     diploid tetraploid aneuploid
  diploid         17          0         1
  tetraploid       0         13         1
  aneuploid        0          0         0
  ```

  ### 예제 3) ctree() 함수 이용하여 반응변수가 연속형인 경우 회귀나무를 통한 예측 수행 : airquality 자료에 대해 의사결정나무를 적합. 반응변수 Ozone이 결측인 자료를 제외한 후 ctree() 함수를 적용
  ```
  > airq <- subset(airquality, !is.na(Ozone))
> head(airq)
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
6    28      NA 14.9   66     5   6
7    23     299  8.6   65     5   7
> airct <- ctree(Ozone ~ ., data=airq) 
> airct

	 Conditional inference tree with 5 terminal nodes

Response:  Ozone 
Inputs:  Solar.R, Wind, Temp, Month, Day 
Number of observations:  116 

1) Temp <= 82; criterion = 1, statistic = 56.086
  2) Wind <= 6.9; criterion = 0.998, statistic = 12.969
    3)*  weights = 10 
  2) Wind > 6.9
    4) Temp <= 77; criterion = 0.997, statistic = 11.599
      5)*  weights = 48 
    4) Temp > 77
      6)*  weights = 21 
1) Temp > 82
  7) Wind <= 10.3; criterion = 0.997, statistic = 11.712
    8)*  weights = 30 
  7) Wind > 10.3
    9)*  weights = 7 
```

#### 💜 최종노드(*로 표시된 마디)가 5개인 트리를 시각화
```
> plot(airct)
```
![image](https://user-images.githubusercontent.com/77424107/198888508-923bdfd9-acbc-41da-89a8-e34ff7bc9604.png)

#### 💜 predict() 함수로 새로운 자료에 대한 예측 수행
```
> head(predict(airct, data=airq))
        Ozone
[1,] 18.47917
[2,] 18.47917
[3,] 18.47917
[4,] 18.47917
[5,] 18.47917
[6,] 18.47917
```

#### 💜 자료가 속하는 해당 최종노드의 번호 출력할 때 : type='node' 옵션 적용
```
> predict(airct, data=airq, type="node")
  [1] 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 3 5 6 9 9 6 5 5 5 5 5 8 8
 [38] 6 8 9 8 8 8 8 5 6 6 3 6 8 8 9 3 8 8 6 9 8 8 8 6 3 6 6 8 8 8 8 8 8 9 6 6 5
 [75] 3 5 6 6 5 5 6 3 8 8 8 8 8 8 8 8 8 8 9 6 6 5 5 6 5 3 5 5 3 5 5 5 6 5 5 6 5
[112] 5 3 5 5 5
```

#### 💜 예측값을 이용하여 평균제곱오차 구하기
```
> mean((airq$Ozone - predict(airct))^2) # mean square error
[1] 403.6668
```

# ⭐ 의사결정나무모형의 장단점

---

## ✅ 장점
- 구조가 단순하여 해석이 용이
- 유용한 입력변수의 파악과 예측변수 간 상호작용 및 비선형성을 고려하여 분석이 수행됨
- 선형성, 정규성, 등분산성 등의 수학적 가정 불필요

## ✅ 단점
- 분류 기준값의 경계선 근방의 자룟값에 대해서는 오차가 클 수 있음(비연속성)
- 로지스틱 회귀와 같이 각 예측변수의 효과를 파악하기 어려움
- 새로운 자료에 대한 예측이 불안정할 수 있음